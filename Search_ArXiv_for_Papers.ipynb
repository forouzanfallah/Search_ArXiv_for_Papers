{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjVkAvxcyk4+RAYateu/VC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forouzanfallah/Search_ArXiv_for_Papers/blob/main/Search_ArXiv_for_Papers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "def search_arxiv_cs(keyword, start_date, end_date):\n",
        "    base_url = \"https://arxiv.org\"\n",
        "    search_url = f\"{base_url}/search/cs?query={keyword}&searchtype=all&abstracts=show&order=-announced_date_first&size=50\"\n",
        "    papers = []\n",
        "\n",
        "    # Send request to the arXiv search page\n",
        "    response = requests.get(search_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find all the paper entries\n",
        "        entries = soup.find_all('li', class_='arxiv-result')\n",
        "        for entry in entries:\n",
        "            # Extract date, title, link, and abstract\n",
        "            date_text = entry.find('p', class_='is-size-7').text.strip()\n",
        "            match = re.search(r'originally announced (\\w+) (\\d{4}).', date_text)\n",
        "            if match:\n",
        "                month, year = match.groups()\n",
        "                date = datetime.datetime.strptime(f\"{month} {year}\", '%B %Y')\n",
        "\n",
        "                # Filter by date\n",
        "                if start_date <= date <= end_date:\n",
        "                    title = entry.find('p', class_='title is-5 mathjax').text.strip()\n",
        "                    identifier = entry.find('p', class_='list-title is-inline-block').a.text.strip()\n",
        "                    identifier = identifier.replace('arXiv:', '').strip()\n",
        "                    link = f\"https://arxiv.org/abs/{identifier}\"\n",
        "                    abstract = entry.find('span', class_='abstract-full has-text-grey-dark mathjax').text.strip()\n",
        "                    abstract = re.sub(r'\\s+', ' ', abstract)  # Replace multiple whitespace with single space\n",
        "\n",
        "                    papers.append({'title': title, 'date': date, 'link': link, 'abstract': abstract})\n",
        "    return papers\n"
      ],
      "metadata": {
        "id": "4jNtPP29Ns3R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "keyword = \"Diffusion Models\"\n",
        "start_date = datetime.datetime(2024, 1, 1)\n",
        "end_date = datetime.datetime(2024, 1, 31)\n",
        "search_results = search_arxiv_cs(keyword, start_date, end_date)\n",
        "\n",
        "# Display the first few results\n",
        "if search_results:\n",
        "    for paper in search_results[:5]:\n",
        "        print(f\"Title: {paper['title']}\\nDate: {paper['date']}\\nLink: {paper['link']}\\nAbstract:\")\n",
        "        sentences = re.split(r'(?<=[.!?]) +', paper['abstract'])\n",
        "        for sentence in sentences:\n",
        "            print(sentence)\n",
        "        print(\"\\n\")\n",
        "else:\n",
        "    print(\"No results found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4lcCgMzOrt4",
        "outputId": "5666f6e9-b8d1-4c05-fb30-989ca3202203"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Synthesizing Moving People with 3D Control\n",
            "Date: 2024-01-01 00:00:00\n",
            "Link: https://arxiv.org/abs/2401.10889\n",
            "Abstract:\n",
            "In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence.\n",
            "Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture.\n",
            "For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image.\n",
            "We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint.\n",
            "Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses.\n",
            "This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions.\n",
            "This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity.\n",
            "In addition to that, the 3D control allows various synthetic camera trajectories to render a person.\n",
            "Our experiments show that our method is resilient in generating prolonged motions and varied challenging and complex poses compared to prior methods.\n",
            "Please check our website for more details: https://boyiliee.github.io/3DHM.github.io/.\n",
            "△ Less\n",
            "\n",
            "\n",
            "Title: ActAnywhere: Subject-Aware Video Background Generation\n",
            "Date: 2024-01-01 00:00:00\n",
            "Link: https://arxiv.org/abs/2401.10822\n",
            "Abstract:\n",
            "Generating video background that tailors to foreground subject motion is an important problem for the movie industry and visual effects community.\n",
            "This task involves synthesizing background that aligns with the motion and appearance of the foreground subject, while also complies with the artist's creative intention.\n",
            "We introduce ActAnywhere, a generative model that automates this process which traditionally requires tedious manual efforts.\n",
            "Our model leverages the power of large-scale video diffusion models, and is specifically tailored for this task.\n",
            "ActAnywhere takes a sequence of foreground subject segmentation as input and an image that describes the desired scene as condition, to produce a coherent video with realistic foreground-background interactions while adhering to the condition frame.\n",
            "We train our model on a large-scale dataset of human-scene interaction videos.\n",
            "Extensive evaluations demonstrate the superior performance of our model, significantly outperforming baselines.\n",
            "Moreover, we show that ActAnywhere generalizes to diverse out-of-distribution samples, including non-human subjects.\n",
            "Please visit our project webpage at https://actanywhere.github.io.\n",
            "△ Less\n",
            "\n",
            "\n",
            "Title: Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion\n",
            "Date: 2024-01-01 00:00:00\n",
            "Link: https://arxiv.org/abs/2401.10786\n",
            "Abstract:\n",
            "Directly generating scenes from satellite imagery offers exciting possibilities for integration into applications like games and map services.\n",
            "However, challenges arise from significant view changes and scene scale.\n",
            "Previous efforts mainly focused on image or video generation, lacking exploration into the adaptability of scene generation for arbitrary views.\n",
            "Existing 3D generation works either operate at the object level or are difficult to utilize the geometry obtained from satellite imagery.\n",
            "To overcome these limitations, we propose a novel architecture for direct 3D scene generation by introducing diffusion models into 3D sparse representations and combining them with neural rendering techniques.\n",
            "Specifically, our approach generates texture colors at the point level for a given geometry using a 3D diffusion model first, which is then transformed into a scene representation in a feed-forward manner.\n",
            "The representation can be utilized to render arbitrary views which would excel in both single-frame quality and inter-frame consistency.\n",
            "Experiments in two city-scale datasets show that our model demonstrates proficiency in generating photo-realistic street-view image sequences and cross-view urban scenes from satellite imagery.\n",
            "△ Less\n",
            "\n",
            "\n",
            "Title: Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model\n",
            "Date: 2024-01-01 00:00:00\n",
            "Link: https://arxiv.org/abs/2401.10700\n",
            "Abstract:\n",
            "Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning.\n",
            "Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined.\n",
            "This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios.\n",
            "An alternative is to enforce the hard constraint of zero violation.\n",
            "However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets.\n",
            "Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset.\n",
            "This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region.\n",
            "Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability.\n",
            "In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning.\n",
            "Thus, we propose a novel energy-guided diffusion model that does not require training a complicated time-dependent classifier to extract the policy, greatly simplifying the training.\n",
            "We compare FISOR against baselines on DSRL benchmark for safe offline RL.\n",
            "Evaluation results show that FISOR is the only method that can guarantee safety satisfaction in all tasks, while achieving top returns in most tasks.\n",
            "△ Less\n",
            "\n",
            "\n",
            "Title: MAEDiff: Masked Autoencoder-enhanced Diffusion Models for Unsupervised Anomaly Detection in Brain Images\n",
            "Date: 2024-01-01 00:00:00\n",
            "Link: https://arxiv.org/abs/2401.10561\n",
            "Abstract:\n",
            "Unsupervised anomaly detection has gained significant attention in the field of medical imaging due to its capability of relieving the costly pixel-level annotation.\n",
            "To achieve this, modern approaches usually utilize generative models to produce healthy references of the diseased images and then identify the abnormalities by comparing the healthy references and the original diseased images.\n",
            "Recently, diffusion models have exhibited promising potential for unsupervised anomaly detection in medical images for their good mode coverage and high sample quality.\n",
            "However, the intrinsic characteristics of the medical images, e.g.\n",
            "the low contrast, and the intricate anatomical structure of the human body make the reconstruction challenging.\n",
            "Besides, the global information of medical images often remain underutilized.\n",
            "To address these two issues, we propose a novel Masked Autoencoder-enhanced Diffusion Model (MAEDiff) for unsupervised anomaly detection in brain images.\n",
            "The MAEDiff involves a hierarchical patch partition.\n",
            "It generates healthy images by overlapping upper-level patches and implements a mechanism based on the masked autoencoders operating on the sub-level patches to enhance the condition on the unnoised regions.\n",
            "Extensive experiments on data of tumors and multiple sclerosis lesions demonstrate the effectiveness of our method.\n",
            "△ Less\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}